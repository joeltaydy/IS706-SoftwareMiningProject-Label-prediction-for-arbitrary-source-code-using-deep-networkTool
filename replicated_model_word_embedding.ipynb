{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import functools\n",
    "import ast\n",
    "import keras\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input,Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D,MaxPool1D,GlobalAveragePooling1D\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import top_k_categorical_accuracy \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, Input, Concatenate, BatchNormalization, MaxPooling1D, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "keras.backend.set_session(session)\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "session = keras.backend.get_session()\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic = lambda x: ast.literal_eval(x)\n",
    "conv = {'Tags': generic}\n",
    "df = pd.read_csv('data/data_final.csv', converters=conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(frac=0.2, random_state=2020)\n",
    "x1 = sample['Body']\n",
    "y1 = sample['Tags']\n",
    "max_words = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words,  lower = True)\n",
    "tokenizer.fit_on_texts(x1)\n",
    "sequences = tokenizer.texts_to_sequences(x1)\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "    \n",
    "# pad sequences\n",
    "max_length=min(200,max([len(s.split()) for s in x1]))\n",
    "x = pad_sequences(sequences, maxlen=max_length, padding='post') #previously was pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit_transform(y1)\n",
    "\n",
    "\n",
    "# saving\n",
    "with open('binarizer', 'wb') as handle:\n",
    "    pickle.dump(multilabel_binarizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "with open('binarizer', 'rb') as handle:\n",
    "    multilabel_binarizer = pickle.load(handle)\n",
    "    \n",
    "y = multilabel_binarizer.transform(y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148964, 342)\n",
      "(148964, 200)\n",
      "502544\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(tokenizer.word_index) + 1\n",
    "print(y.shape)\n",
    "print(x.shape)\n",
    "print(vocab_size)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119171, 200)\n",
      "(14896, 200)\n",
      "(14897, 200)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['states', 'df', 'lt', 'map', 'data', 'state', 'xa', 'states', 'df', 'subset', 'states', 'df', 'group', '8', 'get', 'rid', 'of', 'dc', 'xa', 'states', 'df', 'st', 'lt', 'state', 'abb', 'match', 'states', 'df', 'region', 'tolower', 'state', 'name', 'attach', 'state', 'abbreviations', 'xa', 'xa', 'states', 'df', 'value', 'value', 'states', 'df', 'st', 'xa', 'xa', 'p', 'qplot', 'long', 'lat', 'data', 'states', 'df', 'group', 'group', 'fill', 'value', 'geom', 'polygon', 'xlab', 'ylab', 'main', 'main', 'opts', 'axis', 'text', 'y', 'theme', 'blank', 'axis', 'text', 'x', 'theme', 'blank', 'axis', 'ticks', 'theme', 'blank', 'scale', 'fill', 'continuous', 'name', 'xa', 'p2', 'p', 'geom', 'path', 'data', 'states', 'df', 'color', 'white', 'alpha', '0', '4', 'fill', 'na', 'coord', 'map', 'project', 'xa', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]]\n"
     ]
    }
   ],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "\n",
    "# Creating texts \n",
    "my_texts = list(map(sequence_to_text, [x_train[51]]))\n",
    "print(my_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "# Replicated model\n",
    "inputs = Input(shape=(max_length, ))\n",
    "embedding = Embedding(vocab_size, 16, input_length=max_length, trainable=False)(inputs)\n",
    "convs = []\n",
    "for n, fsz in [(128, 2), (192, 3), (256, 4), (512, 5)]:\n",
    "    model = Conv1D(n, fsz, activation='relu', padding='same')(embedding)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Conv1D(n, fsz, activation='relu', padding='same')(model)\n",
    "    model = MaxPooling1D()(model)\n",
    "    convs.append(model)\n",
    "\n",
    "model = Concatenate(axis=-1)(convs)\n",
    "model = Flatten()(model)\n",
    "## Fully connected layers\n",
    "model = BatchNormalization()(model)\n",
    "model = Dense(7, activation='relu')(model)\n",
    "model = BatchNormalization()(model)\n",
    "model = Dense(7, activation='relu')(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "model = Dense(y_train.shape[1], activation='sigmoid')(model)\n",
    "model = Model(inputs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 16)      8040704     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 200, 128)     4224        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 200, 192)     9408        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 200, 256)     16640       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 200, 512)     41472       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 200, 128)     512         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 200, 192)     768         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 200, 256)     1024        conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 200, 512)     2048        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 200, 128)     32896       batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 200, 192)     110784      batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 200, 256)     262400      batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 200, 512)     1311232     batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 100, 128)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 100, 192)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 100, 256)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 100, 512)     0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 100, 1088)    0           max_pooling1d[0][0]              \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 108800)       0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 108800)       435200      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 7)            761607      batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 7)            28          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 7)            56          batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 7)            28          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 342)          2736        batch_normalization_v1_6[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 11,033,767\n",
      "Trainable params: 2,773,259\n",
      "Non-trainable params: 8,260,508\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_acc = functools.partial(tf.keras.metrics.top_k_categorical_accuracy, k=1)\n",
    "top5_acc = functools.partial(tf.keras.metrics.top_k_categorical_accuracy, k = 5)\n",
    "top1_acc.__name__ = 'top1_acc'\n",
    "top5_acc.__name__ = 'top5_acc'\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    keras.backend.get_session().run(tf.local_variables_initializer())\n",
    "    return auc \n",
    "\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy', 'top_k_categorical_accuracy', top1_acc, top5_acc, auc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119171 samples, validate on 14896 samples\n",
      "WARNING:tensorflow:From F:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "119171/119171 [==============================] - 345s 3ms/sample - loss: 0.0541 - acc: 0.9910 - top_k_categorical_accuracy: 0.2753 - top1_acc: 0.0669 - top5_acc: 0.2753 - auc: 0.6596 - val_loss: 0.0303 - val_acc: 0.9941 - val_top_k_categorical_accuracy: 0.3027 - val_top1_acc: 0.0809 - val_top5_acc: 0.3027 - val_auc: 0.7327\n",
      "Epoch 2/10\n",
      "119171/119171 [==============================] - 339s 3ms/sample - loss: 0.0284 - acc: 0.9942 - top_k_categorical_accuracy: 0.3724 - top1_acc: 0.1436 - top5_acc: 0.3724 - auc: 0.7555 - val_loss: 0.0283 - val_acc: 0.9940 - val_top_k_categorical_accuracy: 0.3964 - val_top1_acc: 0.1843 - val_top5_acc: 0.3964 - val_auc: 0.7732\n",
      "Epoch 3/10\n",
      "119171/119171 [==============================] - 339s 3ms/sample - loss: 0.0272 - acc: 0.9943 - top_k_categorical_accuracy: 0.4170 - top1_acc: 0.1914 - top5_acc: 0.4170 - auc: 0.7844 - val_loss: 0.0278 - val_acc: 0.9940 - val_top_k_categorical_accuracy: 0.4300 - val_top1_acc: 0.2092 - val_top5_acc: 0.4300 - val_auc: 0.7933\n",
      "Epoch 4/10\n",
      "119171/119171 [==============================] - 338s 3ms/sample - loss: 0.0266 - acc: 0.9944 - top_k_categorical_accuracy: 0.4356 - top1_acc: 0.2124 - top5_acc: 0.4356 - auc: 0.7999 - val_loss: 0.0294 - val_acc: 0.9935 - val_top_k_categorical_accuracy: 0.4360 - val_top1_acc: 0.2160 - val_top5_acc: 0.4360 - val_auc: 0.8053\n",
      "Epoch 5/10\n",
      "119171/119171 [==============================] - 338s 3ms/sample - loss: 0.0262 - acc: 0.9944 - top_k_categorical_accuracy: 0.4496 - top1_acc: 0.2219 - top5_acc: 0.4496 - auc: 0.8097 - val_loss: 0.0275 - val_acc: 0.9939 - val_top_k_categorical_accuracy: 0.4387 - val_top1_acc: 0.2238 - val_top5_acc: 0.4387 - val_auc: 0.8136\n",
      "Epoch 6/10\n",
      "119171/119171 [==============================] - 340s 3ms/sample - loss: 0.0259 - acc: 0.9944 - top_k_categorical_accuracy: 0.4589 - top1_acc: 0.2307 - top5_acc: 0.4589 - auc: 0.8168 - val_loss: 0.0279 - val_acc: 0.9938 - val_top_k_categorical_accuracy: 0.4548 - val_top1_acc: 0.2313 - val_top5_acc: 0.4548 - val_auc: 0.8195\n",
      "Epoch 7/10\n",
      "119171/119171 [==============================] - 336s 3ms/sample - loss: 0.0256 - acc: 0.9944 - top_k_categorical_accuracy: 0.4676 - top1_acc: 0.2390 - top5_acc: 0.4676 - auc: 0.8220 - val_loss: 0.0272 - val_acc: 0.9941 - val_top_k_categorical_accuracy: 0.4472 - val_top1_acc: 0.2297 - val_top5_acc: 0.4472 - val_auc: 0.8242\n",
      "Epoch 8/10\n",
      "119171/119171 [==============================] - 347s 3ms/sample - loss: 0.0253 - acc: 0.9945 - top_k_categorical_accuracy: 0.4752 - top1_acc: 0.2485 - top5_acc: 0.4752 - auc: 0.8263 - val_loss: 0.0280 - val_acc: 0.9941 - val_top_k_categorical_accuracy: 0.4317 - val_top1_acc: 0.2217 - val_top5_acc: 0.4317 - val_auc: 0.8280\n",
      "Epoch 9/10\n",
      "119171/119171 [==============================] - 338s 3ms/sample - loss: 0.0251 - acc: 0.9945 - top_k_categorical_accuracy: 0.4833 - top1_acc: 0.2567 - top5_acc: 0.4833 - auc: 0.8297 - val_loss: 0.0271 - val_acc: 0.9941 - val_top_k_categorical_accuracy: 0.4571 - val_top1_acc: 0.2477 - val_top5_acc: 0.4571 - val_auc: 0.8313\n",
      "Epoch 10/10\n",
      "119171/119171 [==============================] - 338s 3ms/sample - loss: 0.0248 - acc: 0.9946 - top_k_categorical_accuracy: 0.4908 - top1_acc: 0.2638 - top5_acc: 0.4908 - auc: 0.8328 - val_loss: 0.0267 - val_acc: 0.9942 - val_top_k_categorical_accuracy: 0.4650 - val_top1_acc: 0.2526 - val_top5_acc: 0.4650 - val_auc: 0.8342\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.backend import set_session\n",
    "from tensorflow.python.keras.models import load_model\n",
    "global session\n",
    "global graph\n",
    "with graph.as_default():\n",
    "    set_session(session)\n",
    "    history = model.fit(x_train, y_train,\n",
    "                          epochs=10,\n",
    "                          batch_size=16,\n",
    "                          verbose=1,\n",
    "                          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save((\"model/replicated_model_word_embedding.h5\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_acc = functools.partial(tf.keras.metrics.top_k_categorical_accuracy, k=1)\n",
    "top5_acc = functools.partial(tf.keras.metrics.top_k_categorical_accuracy, k = 5)\n",
    "top1_acc.__name__ = 'top1_acc'\n",
    "top5_acc.__name__ = 'top5_acc'\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    keras.backend.get_session().run(tf.local_variables_initializer())\n",
    "    return auc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.backend import set_session\n",
    "from tensorflow.python.keras.models import load_model\n",
    "global session\n",
    "global graph\n",
    "with graph.as_default():\n",
    "    set_session(session)\n",
    "    model = tf.keras.models.load_model(\"model/replicated_model_word_embedding.h5\", \n",
    "                                custom_objects={'top1_acc':top1_acc, 'top5_acc':top5_acc, 'auc':auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14897/14897 [==============================] - 12s 786us/sample - loss: 0.0264 - acc: 0.9942 - top_k_categorical_accuracy: 0.4711 - top1_acc: 0.2543 - top5_acc: 0.4711 - auc: 0.8474\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.02638202022283223\n",
      "Test acc: 0.9942439794540405\n",
      "Test top_k_categorical_accuracy: 0.4711010456085205\n",
      "Test top1_acc: 0.2543431222438812\n",
      "Test top5_acc: 0.4711010456085205\n",
      "Test auc: 0.847448468208313\n"
     ]
    }
   ],
   "source": [
    "for i in range (len(model.metrics_names)):\n",
    "    print(\"Test {}: {}\".format(model.metrics_names[i],score[i]) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
