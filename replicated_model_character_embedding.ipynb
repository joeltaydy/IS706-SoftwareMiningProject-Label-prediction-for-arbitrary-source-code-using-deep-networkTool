{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools\n",
    "import ast\n",
    "import keras\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input,Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D,MaxPool1D,GlobalAveragePooling1D\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import top_k_categorical_accuracy \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, Input, Concatenate, BatchNormalization, MaxPooling1D, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "keras.backend.set_session(session)\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "session = keras.backend.get_session()\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic = lambda x: ast.literal_eval(x)\n",
    "conv = {'Tags': generic}\n",
    "df = pd.read_csv('data/data_final.csv', converters=conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(frac=0.2, random_state=2020)\n",
    "x1 = sample['Body']\n",
    "y1 = sample['Tags']\n",
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_table = sorted(list(set(''.join(df['Body'].values))))\n",
    "vocab_size = len(char_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = x1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "for enum, code in enumerate(x2):\n",
    "    try:\n",
    "        code_char =[]\n",
    "        for char in code[:max_length]:\n",
    "            if char in char_table:\n",
    "                code_char.append(char_table.index(char))\n",
    "        code_char_index = code_char + ([0] * (max_length - len(code)))\n",
    "        xs.append(np.array(code_char_index))\n",
    "    except:\n",
    "        print(enum, code)\n",
    "x = np.array(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit_transform(y1)\n",
    "\n",
    "# saving\n",
    "with open('binarizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(multilabel_binarizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "with open('binarizer.pickle', 'rb') as handle:\n",
    "    multilabel_binarizer = pickle.load(handle)\n",
    "\n",
    "y = multilabel_binarizer.transform(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148964, 342)\n",
      "(148964, 200)\n",
      "4435\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(x.shape)\n",
    "print(vocab_size)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119171, 200)\n",
      "(14896, 200)\n",
      "(14897, 200)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "WARNING:tensorflow:From F:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#Replicated model\n",
    "inputs = Input(shape=(max_length, ))\n",
    "embedding = Embedding(vocab_size, 16, input_length=max_length, trainable=False)(inputs)\n",
    "convs = []\n",
    "for n, fsz in [(128, 2), (192, 3), (256, 4), (512, 5)]:\n",
    "    model = Conv1D(n, fsz, activation='relu', padding='same')(embedding)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Conv1D(n, fsz, activation='relu', padding='same')(model)\n",
    "    model = MaxPooling1D()(model)\n",
    "    convs.append(model)\n",
    "\n",
    "model = Concatenate(axis=-1)(convs)\n",
    "model = Flatten()(model)\n",
    "## Fully connected layers\n",
    "model = BatchNormalization()(model)\n",
    "model = Dense(7, activation='relu')(model)\n",
    "model = BatchNormalization()(model)\n",
    "model = Dense(7, activation='relu')(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "model = Dense(y_train.shape[1], activation='sigmoid')(model)\n",
    "model = Model(inputs, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 16)      70960       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 200, 128)     4224        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 200, 192)     9408        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 200, 256)     16640       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 200, 512)     41472       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 200, 128)     512         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 200, 192)     768         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 200, 256)     1024        conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 200, 512)     2048        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 200, 128)     32896       batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 200, 192)     110784      batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 200, 256)     262400      batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 200, 512)     1311232     batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 100, 128)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 100, 192)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 100, 256)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 100, 512)     0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 100, 1088)    0           max_pooling1d[0][0]              \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 108800)       0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 108800)       435200      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 7)            761607      batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 7)            28          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 7)            56          batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 7)            28          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 342)          2736        batch_normalization_v1_6[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 3,064,023\n",
      "Trainable params: 2,773,259\n",
      "Non-trainable params: 290,764\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\metrics_impl.py:526: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From F:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\metrics_impl.py:788: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "top1_acc = functools.partial(tf.keras.metrics.top_k_categorical_accuracy, k=1)\n",
    "top5_acc = functools.partial(tf.keras.metrics.top_k_categorical_accuracy, k = 5)\n",
    "top1_acc.__name__ = 'top1_acc'\n",
    "top5_acc.__name__ = 'top5_acc'\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    keras.backend.get_session().run(tf.local_variables_initializer())\n",
    "    return auc \n",
    "\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy', 'top_k_categorical_accuracy', top1_acc, top5_acc, auc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119171 samples, validate on 14896 samples\n",
      "WARNING:tensorflow:From F:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "119171/119171 [==============================] - 351s 3ms/sample - loss: 0.0532 - acc: 0.9901 - top_k_categorical_accuracy: 0.3227 - top1_acc: 0.1102 - top5_acc: 0.3227 - auc: 0.6655 - val_loss: 0.0287 - val_acc: 0.9939 - val_top_k_categorical_accuracy: 0.3841 - val_top1_acc: 0.1427 - val_top5_acc: 0.3841 - val_auc: 0.7492\n",
      "Epoch 2/10\n",
      "119171/119171 [==============================] - 348s 3ms/sample - loss: 0.0270 - acc: 0.9943 - top_k_categorical_accuracy: 0.4375 - top1_acc: 0.2021 - top5_acc: 0.4375 - auc: 0.7749 - val_loss: 0.0264 - val_acc: 0.9942 - val_top_k_categorical_accuracy: 0.4630 - val_top1_acc: 0.2328 - val_top5_acc: 0.4630 - val_auc: 0.7932\n",
      "Epoch 3/10\n",
      "119171/119171 [==============================] - 355s 3ms/sample - loss: 0.0252 - acc: 0.9944 - top_k_categorical_accuracy: 0.4953 - top1_acc: 0.2581 - top5_acc: 0.4953 - auc: 0.8057 - val_loss: 0.0277 - val_acc: 0.9938 - val_top_k_categorical_accuracy: 0.4545 - val_top1_acc: 0.2317 - val_top5_acc: 0.4545 - val_auc: 0.8154\n",
      "Epoch 4/10\n",
      "119171/119171 [==============================] - 353s 3ms/sample - loss: 0.0243 - acc: 0.9945 - top_k_categorical_accuracy: 0.5206 - top1_acc: 0.2825 - top5_acc: 0.5206 - auc: 0.8228 - val_loss: 0.0242 - val_acc: 0.9945 - val_top_k_categorical_accuracy: 0.5290 - val_top1_acc: 0.3024 - val_top5_acc: 0.5290 - val_auc: 0.8293\n",
      "Epoch 5/10\n",
      "119171/119171 [==============================] - 338s 3ms/sample - loss: 0.0237 - acc: 0.9946 - top_k_categorical_accuracy: 0.5402 - top1_acc: 0.2976 - top5_acc: 0.5402 - auc: 0.8348 - val_loss: 0.0242 - val_acc: 0.9945 - val_top_k_categorical_accuracy: 0.5302 - val_top1_acc: 0.3065 - val_top5_acc: 0.5302 - val_auc: 0.8393\n",
      "Epoch 6/10\n",
      "119171/119171 [==============================] - 338s 3ms/sample - loss: 0.0231 - acc: 0.9947 - top_k_categorical_accuracy: 0.5562 - top1_acc: 0.3116 - top5_acc: 0.5562 - auc: 0.8434 - val_loss: 0.0247 - val_acc: 0.9943 - val_top_k_categorical_accuracy: 0.5375 - val_top1_acc: 0.3073 - val_top5_acc: 0.5375 - val_auc: 0.8468\n",
      "Epoch 7/10\n",
      "119171/119171 [==============================] - 338s 3ms/sample - loss: 0.0227 - acc: 0.9948 - top_k_categorical_accuracy: 0.5685 - top1_acc: 0.3225 - top5_acc: 0.5685 - auc: 0.8500 - val_loss: 0.0243 - val_acc: 0.9944 - val_top_k_categorical_accuracy: 0.5510 - val_top1_acc: 0.3167 - val_top5_acc: 0.5510 - val_auc: 0.8528\n",
      "Epoch 8/10\n",
      "119171/119171 [==============================] - 338s 3ms/sample - loss: 0.0223 - acc: 0.9948 - top_k_categorical_accuracy: 0.5792 - top1_acc: 0.3311 - top5_acc: 0.5792 - auc: 0.8554 - val_loss: 0.0243 - val_acc: 0.9944 - val_top_k_categorical_accuracy: 0.5552 - val_top1_acc: 0.3165 - val_top5_acc: 0.5552 - val_auc: 0.8578\n",
      "Epoch 9/10\n",
      "119171/119171 [==============================] - 338s 3ms/sample - loss: 0.0219 - acc: 0.9948 - top_k_categorical_accuracy: 0.5894 - top1_acc: 0.3383 - top5_acc: 0.5894 - auc: 0.8601 - val_loss: 0.0240 - val_acc: 0.9946 - val_top_k_categorical_accuracy: 0.5524 - val_top1_acc: 0.3147 - val_top5_acc: 0.5524 - val_auc: 0.8621\n",
      "Epoch 10/10\n",
      "119171/119171 [==============================] - 338s 3ms/sample - loss: 0.0216 - acc: 0.9949 - top_k_categorical_accuracy: 0.5972 - top1_acc: 0.3436 - top5_acc: 0.5972 - auc: 0.8641 - val_loss: 0.0243 - val_acc: 0.9944 - val_top_k_categorical_accuracy: 0.5551 - val_top1_acc: 0.3108 - val_top5_acc: 0.5551 - val_auc: 0.8659\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.backend import set_session\n",
    "from tensorflow.python.keras.models import load_model\n",
    "global session\n",
    "global graph\n",
    "with graph.as_default():\n",
    "    set_session(session)\n",
    "    history = model.fit(x_train, y_train,\n",
    "                          epochs=10,\n",
    "                          batch_size=16,\n",
    "                          verbose=1,\n",
    "                          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model/replicated_model_char_embedding.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_acc = functools.partial(tf.keras.metrics.top_k_categorical_accuracy, k=1)\n",
    "top5_acc = functools.partial(tf.keras.metrics.top_k_categorical_accuracy, k = 5)\n",
    "top1_acc.__name__ = 'top1_acc'\n",
    "top5_acc.__name__ = 'top5_acc'\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    keras.backend.get_session().run(tf.local_variables_initializer())\n",
    "    return auc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.backend import set_session\n",
    "from tensorflow.python.keras.models import load_model\n",
    "global session\n",
    "global graph\n",
    "with graph.as_default():\n",
    "    set_session(session)\n",
    "    model = tf.keras.models.load_model(\"model/replicated_model_char_embedding.h5\", \n",
    "                                custom_objects={'top1_acc':top1_acc, 'top5_acc':top5_acc, 'auc':auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14897/14897 [==============================] - 15s 986us/sample - loss: 0.0242 - acc: 0.9945 - top_k_categorical_accuracy: 0.5500 - top1_acc: 0.3029 - top5_acc: 0.5500 - auc: 0.8724\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.02418637656802226\n",
      "Test acc: 0.9944603443145752\n",
      "Test top_k_categorical_accuracy: 0.5499716401100159\n",
      "Test top1_acc: 0.30287885665893555\n",
      "Test top5_acc: 0.5499716401100159\n",
      "Test auc: 0.8723740577697754\n"
     ]
    }
   ],
   "source": [
    "for i in range (len(model.metrics_names)):\n",
    "    print(\"Test {}: {}\".format(model.metrics_names[i],score[i]) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
